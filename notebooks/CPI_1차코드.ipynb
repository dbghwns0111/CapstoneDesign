{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5kfg2U8mnCc"
      },
      "source": [
        "# data íŒŒì¼ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqLYf5f-l4h9",
        "outputId": "807cc352-b27d-4d7e-cba6-f91273abb7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fredapi in c:\\users\\user\\anaconda3\\lib\\site-packages (0.5.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from fredapi) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->fredapi) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->fredapi) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->fredapi) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# install fred library\n",
        "!pip install fredapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "No-wMPJBl28A"
      },
      "outputs": [],
      "source": [
        "# import labrary\n",
        "from fredapi import Fred\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NwYLEDmylgWT"
      },
      "outputs": [],
      "source": [
        "# FRED API KEY\n",
        "fred = Fred(api_key='7faa1595daf7911ed2a907d11dc20a2d')\n",
        "\n",
        "# ë‹¤ìš´ë¡œë“œ ê¸°ê°„ ì„¤ì •\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2021-11-30'\n",
        "\n",
        "# ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì •ì˜\n",
        "fred_ids = [\n",
        "    \"KORCPIALLMINMEI\", \"KORCP010000IXOBM\", \"KORCP020000IXOBM\", \"KORCP030000IXOBM\", \"KORCP040000IXOBM\",\n",
        "    \"KORCP050000IXOBM\", \"KORCP060000IXOBM\",\"KORCP070000IXOBM\", \"KORCP080000IXOBM\", \"KORCP090000IXOBM\",\n",
        "    \"KORCP100000IXOBM\", \"KORCP110000IXOBM\", \"KORCP120000IXOBM\", \"KORCPICORMINMEI\", \"KORCPIENGMINMEI\",\n",
        "    \"KORCPGRSE01IXOBM\", \"KORCPGRLH02IXOBM\", \"KORCPGRHO02IXOBM\", \"KORCP040100IXOBM\", \"KORCP040400IXOBM\",\n",
        "    \"KORCP040500IXOBM\", \"KORCP040300IXOBM\"\n",
        "]\n",
        "\n",
        "# ê° ë¦¬ìŠ¤íŠ¸ ì— ëŒ€í•œ ì‹œë¦¬ì¦ˆ IDì™€ ì´ë¦„ ë§¤í•‘\n",
        "column_rename_map = {\n",
        "    \"KORCPIALLMINMEI\": \"Total CPI\",\n",
        "    \"KORCP010000IXOBM\": \"Food and non-alcoholic beverages\",\n",
        "    \"KORCP020000IXOBM\": \"Alcoholic beverages, tobacco and narcotics\",\n",
        "    \"KORCP030000IXOBM\": \"Clothing and footwear\",\n",
        "    \"KORCP040000IXOBM\": \"Housing, water, electricity, and fuel\",\n",
        "    \"KORCP050000IXOBM\": \"Household goods and services\",\n",
        "    \"KORCP060000IXOBM\": \"Health\",\n",
        "    \"KORCP070000IXOBM\": \"Transportation\",\n",
        "    \"KORCP080000IXOBM\": \"Communication\",\n",
        "    \"KORCP090000IXOBM\": \"Recreation and culture\",\n",
        "    \"KORCP100000IXOBM\": \"Education\",\n",
        "    \"KORCP110000IXOBM\": \"Restaurants and hotels\",\n",
        "    \"KORCP120000IXOBM\": \"Miscellaneous goods and services\",\n",
        "    \"KORCPICORMINMEI\": \"All items (non-food non-energy)\",\n",
        "    \"KORCPIENGMINMEI\": \"Energy\",\n",
        "    \"KORCPGRSE01IXOBM\": \"Services\",\n",
        "    \"KORCPGRLH02IXOBM\": \"Services less housing\",\n",
        "    \"KORCPGRHO02IXOBM\": \"Housing excluding imputed rentals for housing\",\n",
        "    \"KORCP040100IXOBM\": \"Actual rentals for housing\",\n",
        "    \"KORCP040400IXOBM\": \"Water supply and misc. services relating to dwelling\",\n",
        "    \"KORCP040500IXOBM\": \"Electricity, gas and other fuels\",\n",
        "    \"KORCP040300IXOBM\": \"Maintenance and repair of the dwelling\"\n",
        "}\n",
        "\n",
        "# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”\n",
        "data = pd.DataFrame()\n",
        "\n",
        "# ê° ì‹œë¦¬ì¦ˆ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì´ë¦„ ë³€ê²½\n",
        "for fred_id in fred_ids:\n",
        "    try:\n",
        "        series_data = fred.get_series(fred_id, start_date, end_date)\n",
        "        series_data = series_data.rename(column_rename_map[fred_id])\n",
        "        data = pd.concat([data, series_data], axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {fred_id}: {e}\")\n",
        "\n",
        "# ì¸ë±ìŠ¤ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "data.index = pd.to_datetime(data.index)\n",
        "data.index.name = 'Date'\n",
        "\n",
        "# ì €ì¥ í´ë” ìƒì„± (data/raw)\n",
        "output_dir = '../data/raw'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# csvíŒŒì¼ ì €ì¥\n",
        "output_path = os.path.join(output_dir, 'fred_data_2010_2021.csv')\n",
        "data.to_csv(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z4_lTlLmuE1"
      },
      "source": [
        "# StandardScaler()ë¡œ ìŠ¤ì¼€ì¼ë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd6KGvJrm2WC",
        "outputId": "e79f7a5f-6960-4117-9198-93d3fc74ef45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ì €ì¥ ì™„ë£Œ: ../data/lasso_results/lasso_importance1.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1632\\1087459845.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        }
      ],
      "source": [
        "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"../data/raw/fred_data_2010_2021.csv\")\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.set_index(\"Date\")\n",
        "\n",
        "# 2. ëŒ€ìƒ ë³€ìˆ˜ ì„¤ì •\n",
        "target_col = \"Total CPI\"\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ & ìŠ¤ì¼€ì¼ë§\n",
        "X = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 4. LassoCV ëª¨ë¸ í•™ìŠµ\n",
        "lasso = LassoCV(cv=5, random_state=42)\n",
        "lasso.fit(X_scaled, y)\n",
        "\n",
        "# 5. ì¤‘ìš”ë„ ì¶”ì¶œ\n",
        "coefs = pd.Series(lasso.coef_, index=X.columns)\n",
        "non_zero_coefs = coefs[coefs != 0]\n",
        "\n",
        "# ì¤‘ìš”ë„ ê³„ì‚°: ì ˆëŒ“ê°’ ë¹„ìœ¨\n",
        "importance = np.abs(non_zero_coefs)\n",
        "importance_pct = (importance / importance.sum()) * 100\n",
        "\n",
        "# 6. ê²°ê³¼ ì •ë¦¬ (ì¤‘ìš”ë„ ë†’ì€ ìˆœ ì •ë ¬)\n",
        "result_df = pd.DataFrame({\n",
        "    \"feature\": importance_pct.index,\n",
        "    \"coef\": non_zero_coefs.values,\n",
        "    \"importance\": importance_pct.values\n",
        "})\n",
        "result_df = result_df.sort_values(by=\"importance\", ascending=False).reset_index(drop=True)\n",
        "result_df[\"rank\"] = result_df.index + 1\n",
        "result_df = result_df[[\"rank\", \"feature\", \"coef\", \"importance\"]]\n",
        "\n",
        "# 7. ì €ì¥\n",
        "output_dir = \"../data/lasso_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "result_df.to_csv(f\"{output_dir}/lasso_importance1.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_dir}/lasso_importance1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glcQpqrhm74d"
      },
      "source": [
        "# MinMaxScaler()ë¡œ ìŠ¤ì¼€ì¼ë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwtpDeO2m80J",
        "outputId": "d3762421-631e-49a9-93d4-8542f45bbce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ë…¼ë¬¸ ì¡°ê±´ìœ¼ë¡œ Lasso ë¶„ì„ ì™„ë£Œ! ğŸ‘‰ ì €ì¥ ê²½ë¡œ: ../data/lasso_results/lasso_importance2.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1632\\998128824.py:15: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        }
      ],
      "source": [
        "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"../data/raw/fred_data_2010_2021.csv\")\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.set_index(\"Date\")\n",
        "\n",
        "# 2. ë¶„ì„ ëŒ€ìƒ ì‹œê¸°ë§Œ í•„í„°ë§ (2010.01 ~ 2021.11)\n",
        "df = df.loc[\"2010-01-01\":\"2021-11-01\"]\n",
        "\n",
        "# 3. íƒ€ê²Ÿ/ì…ë ¥ ë³€ìˆ˜ ë¶„ë¦¬\n",
        "target_col = \"Total CPI\"\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# 4. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "X = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "\n",
        "# 5. Min-Max ì •ê·œí™” (ë…¼ë¬¸ ë°©ì‹)\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 6. Lasso ëª¨ë¸ í•™ìŠµ (ë…¼ë¬¸ì—” Î± ë¯¸ì–¸ê¸‰ â†’ ì ì ˆí•œ ê°’ ìˆ˜ë™ ì„¤ì •)\n",
        "lasso = Lasso(alpha=0.001, max_iter=10000)  # í•„ìš” ì‹œ Î± ì¡°ì • ê°€ëŠ¥\n",
        "lasso.fit(X_scaled, y)\n",
        "\n",
        "# 7. ì¤‘ìš”ë„ ê³„ì‚°\n",
        "coefs = pd.Series(lasso.coef_, index=X.columns)\n",
        "non_zero_coefs = coefs[coefs != 0]\n",
        "\n",
        "importance = np.abs(non_zero_coefs)\n",
        "importance_pct = (importance / importance.sum()) * 100\n",
        "\n",
        "# 8. ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥\n",
        "result_df = pd.DataFrame({\n",
        "    \"feature\": importance_pct.index,\n",
        "    \"coef\": non_zero_coefs.values,\n",
        "    \"importance\": importance_pct.values\n",
        "})\n",
        "result_df = result_df.sort_values(by=\"importance\", ascending=False).reset_index(drop=True)\n",
        "result_df[\"rank\"] = result_df.index + 1\n",
        "result_df = result_df[[\"rank\", \"feature\", \"coef\", \"importance\"]]\n",
        "\n",
        "# 9. ì €ì¥\n",
        "output_dir = \"../data/lasso_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "result_df.to_csv(f\"{output_dir}/lasso_importance2.csv\", index=False)\n",
        "\n",
        "print(\"âœ… ë…¼ë¬¸ ì¡°ê±´ìœ¼ë¡œ Lasso ë¶„ì„ ì™„ë£Œ! ğŸ‘‰ ì €ì¥ ê²½ë¡œ:\", f\"{output_dir}/lasso_importance2.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMPoY-tAnGu_"
      },
      "source": [
        "# 1ì°¨ ìµœì¢… ì½”ë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmzzJ_yQnH3G",
        "outputId": "963aa303-524f-4715-e2d5-168e0bbdb333"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1632\\1918399297.py:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X = df.drop(columns=[target_col]).fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Train Loss: 0.0391 | Val Loss: 0.0017\n",
            "Epoch 2/50 | Train Loss: 0.0083 | Val Loss: 0.0003\n",
            "Epoch 3/50 | Train Loss: 0.0054 | Val Loss: 0.0002\n",
            "Epoch 4/50 | Train Loss: 0.0046 | Val Loss: 0.0004\n",
            "Epoch 5/50 | Train Loss: 0.0038 | Val Loss: 0.0002\n",
            "Epoch 6/50 | Train Loss: 0.0035 | Val Loss: 0.0007\n",
            "Epoch 7/50 | Train Loss: 0.0031 | Val Loss: 0.0004\n",
            "Epoch 8/50 | Train Loss: 0.0027 | Val Loss: 0.0003\n",
            "Epoch 9/50 | Train Loss: 0.0028 | Val Loss: 0.0004\n",
            "Epoch 10/50 | Train Loss: 0.0025 | Val Loss: 0.0002\n",
            "Epoch 11/50 | Train Loss: 0.0022 | Val Loss: 0.0009\n",
            "Epoch 12/50 | Train Loss: 0.0024 | Val Loss: 0.0007\n",
            "Epoch 13/50 | Train Loss: 0.0026 | Val Loss: 0.0008\n",
            "Epoch 14/50 | Train Loss: 0.0021 | Val Loss: 0.0003\n",
            "Epoch 15/50 | Train Loss: 0.0018 | Val Loss: 0.0006\n",
            "Epoch 16/50 | Train Loss: 0.0019 | Val Loss: 0.0002\n",
            "Epoch 17/50 | Train Loss: 0.0017 | Val Loss: 0.0001\n",
            "Epoch 18/50 | Train Loss: 0.0017 | Val Loss: 0.0001\n",
            "Epoch 19/50 | Train Loss: 0.0017 | Val Loss: 0.0004\n",
            "Epoch 20/50 | Train Loss: 0.0016 | Val Loss: 0.0004\n",
            "Epoch 21/50 | Train Loss: 0.0016 | Val Loss: 0.0002\n",
            "Epoch 22/50 | Train Loss: 0.0015 | Val Loss: 0.0004\n",
            "Epoch 23/50 | Train Loss: 0.0018 | Val Loss: 0.0002\n",
            "Epoch 24/50 | Train Loss: 0.0016 | Val Loss: 0.0009\n",
            "Epoch 25/50 | Train Loss: 0.0016 | Val Loss: 0.0003\n",
            "Epoch 26/50 | Train Loss: 0.0014 | Val Loss: 0.0003\n",
            "Epoch 27/50 | Train Loss: 0.0013 | Val Loss: 0.0003\n",
            "Epoch 28/50 | Train Loss: 0.0015 | Val Loss: 0.0001\n",
            "Epoch 29/50 | Train Loss: 0.0013 | Val Loss: 0.0001\n",
            "Epoch 30/50 | Train Loss: 0.0012 | Val Loss: 0.0002\n",
            "Epoch 31/50 | Train Loss: 0.0012 | Val Loss: 0.0002\n",
            "Epoch 32/50 | Train Loss: 0.0012 | Val Loss: 0.0002\n",
            "Epoch 33/50 | Train Loss: 0.0011 | Val Loss: 0.0002\n",
            "Epoch 34/50 | Train Loss: 0.0012 | Val Loss: 0.0002\n",
            "Epoch 35/50 | Train Loss: 0.0012 | Val Loss: 0.0001\n",
            "Epoch 36/50 | Train Loss: 0.0011 | Val Loss: 0.0001\n",
            "Epoch 37/50 | Train Loss: 0.0011 | Val Loss: 0.0001\n",
            "Epoch 38/50 | Train Loss: 0.0010 | Val Loss: 0.0002\n",
            "Epoch 39/50 | Train Loss: 0.0013 | Val Loss: 0.0006\n",
            "Epoch 40/50 | Train Loss: 0.0011 | Val Loss: 0.0002\n",
            "Epoch 41/50 | Train Loss: 0.0011 | Val Loss: 0.0001\n",
            "Epoch 42/50 | Train Loss: 0.0010 | Val Loss: 0.0001\n",
            "Epoch 43/50 | Train Loss: 0.0009 | Val Loss: 0.0001\n",
            "Epoch 44/50 | Train Loss: 0.0010 | Val Loss: 0.0002\n",
            "Epoch 45/50 | Train Loss: 0.0010 | Val Loss: 0.0001\n",
            "Epoch 46/50 | Train Loss: 0.0009 | Val Loss: 0.0005\n",
            "Epoch 47/50 | Train Loss: 0.0011 | Val Loss: 0.0004\n",
            "Epoch 48/50 | Train Loss: 0.0010 | Val Loss: 0.0002\n",
            "Epoch 49/50 | Train Loss: 0.0010 | Val Loss: 0.0002\n",
            "Epoch 50/50 | Train Loss: 0.0008 | Val Loss: 0.0001\n",
            "\n",
            "ğŸ“Š ì •ê·œí™” ìƒíƒœ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:\n",
            "âœ… RMSE   : 0.0099\n",
            "âœ… MAE    : 0.0080\n",
            "âœ… MAPE   : 1.71%\n",
            "âœ… SMAPE  : 1.74%\n",
            "âœ… NRMSE  : 0.0495\n",
            "âœ… RÂ²     : 0.9976\n",
            "âœ… MSE    : 0.000097\n"
          ]
        }
      ],
      "source": [
        "# cpi_forecasting_pipeline.py (ì •ê·œí™” íë¦„ ìˆ˜ì • ë°˜ì˜)\n",
        "\n",
        "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# âœ… íŒŒë¼ë¯¸í„°\n",
        "PAST_STEPS = 310\n",
        "FUTURE_STEPS = 31\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "\n",
        "# âœ… Lasso ê¸°ë°˜ ë³€ìˆ˜ ì„ íƒ & ì €ì¥\n",
        "raw_df = pd.read_csv(\"../data/raw/fred_data_2010_2021.csv\", parse_dates=[\"Date\"]).set_index(\"Date\")\n",
        "df = raw_df.loc[\"2010-01-01\":\"2021-11-01\"]\n",
        "target_col = \"Total CPI\"\n",
        "X = df.drop(columns=[target_col]).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "y = df[target_col]\n",
        "\n",
        "scaler_lasso = MinMaxScaler()\n",
        "X_scaled = scaler_lasso.fit_transform(X)\n",
        "lasso = Lasso(alpha=0.001, max_iter=10000)\n",
        "lasso.fit(X_scaled, y)\n",
        "\n",
        "coefs = pd.Series(lasso.coef_, index=X.columns)\n",
        "non_zero = coefs[coefs != 0]\n",
        "importance = np.abs(non_zero)\n",
        "importance_pct = (importance / importance.sum()) * 100\n",
        "selected_features = importance_pct.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "result_df = pd.DataFrame({\n",
        "    \"feature\": importance_pct.index,\n",
        "    \"coef\": non_zero.values,\n",
        "    \"importance\": importance_pct.values\n",
        "}).sort_values(by=\"importance\", ascending=False).reset_index(drop=True)\n",
        "result_df[\"rank\"] = result_df.index + 1\n",
        "result_df = result_df[[\"rank\", \"feature\", \"coef\", \"importance\"]]\n",
        "\n",
        "os.makedirs(\"../data/lasso_results\", exist_ok=True)\n",
        "result_df.to_csv(\"../data/lasso_results/lasso_importance2.csv\", index=False)\n",
        "\n",
        "# âœ… ì„ í˜• ë³´ê°„ ë° ì •ê·œí™”\n",
        "selected_cols = [\"Total CPI\"] + selected_features\n",
        "df = raw_df[selected_cols].copy()\n",
        "df_daily = df.resample(\"D\").interpolate(method=\"linear\")\n",
        "scaler = MinMaxScaler()\n",
        "df_daily_scaled = pd.DataFrame(scaler.fit_transform(df_daily), columns=df_daily.columns, index=df_daily.index)\n",
        "os.makedirs(\"../data/augmented\", exist_ok=True)\n",
        "df_daily_scaled.to_csv(\"../data/augmented/cpi_daily_interpolated.csv\")\n",
        "\n",
        "# âœ… ìŠ¬ë¼ì´ë”© ìœˆë„ìš° êµ¬ì„±\n",
        "data = df_daily_scaled.values\n",
        "X_list, Y_list = [], []\n",
        "for i in range(len(data) - PAST_STEPS - FUTURE_STEPS):\n",
        "    X_list.append(data[i:i+PAST_STEPS])\n",
        "    Y_list.append(data[i+PAST_STEPS:i+PAST_STEPS+FUTURE_STEPS, 0])\n",
        "X_np = np.array(X_list)\n",
        "Y_np = np.array(Y_list)\n",
        "\n",
        "# âœ… Dataset & Dataloader ì •ì˜\n",
        "class CPITimeSeriesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.Y[idx]\n",
        "\n",
        "dataset = CPITimeSeriesDataset(X_np, Y_np)\n",
        "train_len = int(len(dataset)*0.7)\n",
        "val_len = int(len(dataset)*0.15)\n",
        "test_len = len(dataset) - train_len - val_len\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_len, val_len, test_len])\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_set, batch_size=1)\n",
        "\n",
        "# âœ… CNN-LSTM ëª¨ë¸ ì •ì˜\n",
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, input_features, past_steps=PAST_STEPS, future_steps=FUTURE_STEPS, hidden_dim=512, kernel_size=3, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_features, hidden_dim, kernel_size)\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, future_steps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.pool(self.conv2(self.conv1(x)))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        return self.fc(x)\n",
        "\n",
        "# âœ… í•™ìŠµ ë° í…ŒìŠ¤íŠ¸\n",
        "model = CNNLSTM(input_features=X_np.shape[2]).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "device = next(model.parameters()).device\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses, val_losses = [], []\n",
        "        for Xb, Yb in train_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(Xb), Yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for Xb, Yb in val_loader:\n",
        "                Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "                val_losses.append(criterion(model(Xb), Yb).item())\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {np.mean(val_losses):.4f}\")\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, Yb in test_loader:\n",
        "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "            preds.append(model(Xb).cpu().numpy())\n",
        "            trues.append(Yb.cpu().numpy())\n",
        "    return np.concatenate(preds), np.concatenate(trues)\n",
        "\n",
        "train(model, train_loader, val_loader, epochs=EPOCHS)\n",
        "preds, trues = test(model, test_loader)\n",
        "\n",
        "# âœ… ì •ê·œí™” ìƒíƒœ ì„±ëŠ¥ í‰ê°€\n",
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
        "\n",
        "def nrmse(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return rmse / np.std(y_true)\n",
        "\n",
        "y_true = trues.flatten()\n",
        "y_pred = preds.flatten()\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mape_val = mape(y_true, y_pred)\n",
        "smape_val = smape(y_true, y_pred)\n",
        "nrmse_val = nrmse(y_true, y_pred)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "mse_val = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "print(\"\\nğŸ“Š ì •ê·œí™” ìƒíƒœ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:\")\n",
        "print(f\"âœ… RMSE   : {rmse:.4f}\")\n",
        "print(f\"âœ… MAE    : {mae:.4f}\")\n",
        "print(f\"âœ… MAPE   : {mape_val:.2f}%\")\n",
        "print(f\"âœ… SMAPE  : {smape_val:.2f}%\")\n",
        "print(f\"âœ… NRMSE  : {nrmse_val:.4f}\")\n",
        "print(f\"âœ… RÂ²     : {r2:.4f}\")\n",
        "print(f\"âœ… MSE    : {mse_val:.6f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f5kfg2U8mnCc",
        "1Z4_lTlLmuE1",
        "glcQpqrhm74d"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
